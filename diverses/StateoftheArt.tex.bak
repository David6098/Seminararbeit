\chapter{State of the Art}
Einer der momentan besten Versuche eine künstliche Intelligenz im Bereich Computerspiele zu trainieren besteht darin mehrere Lerntechniken zu verbinden. Einer der derzeitig besten Ansätze ist es bestärkendes Lernen und tiefe Neuronale Netze zu verbinden. Durch das tiefen Neuronale Netz wird mit jeder Ebene die Inputs weiter abstrahiert. Das Problem mit bestärkendem Lernen besteht im Zusammenhang mit Neuronalen Netzen darin, dass es instabil ist, da eine kleine Veränderung in der \textit{action-value Funktion} zu einer großen Veränderung in dem Verhalten des Neuronalen Netzwerks spielen und der Zusammenhängen zwischen Aktion und der Beobachtung die durch die Aktion beeinflusst wurde.Das erste Problem kann dadurch gemindert werden, dass die \textit{Aktions-Werte} nur periodisch den \textit{Ziel-Werten} angeglichen werden.  Das zweite Problem kann behoben werden, indem der Mechanismus \glqq{}experience replay\grqq{}angewendet wird, der die Daten vermischt und so die Zusammenhänge entfernt.
Im Bezug auf alternative Methoden, wie \textit{neural fitted Q-iteration}\cite{riedmiller2005neural}, hat diese Methode den Vorteil, dass sie deutlich effizienter große Neuronale Netze trainieren.\\ Die Ergebnisse die durch diese Methode erreicht werden können sind besser besser als Level die ein menschlicher, professioneller Spieletester(100\%) erreicht (bei Space Invaders 121\% oder Video Pinball 2539\%). Es hat aber auch Grenzen wie bei Montezuma's Revenge ( 0\%),da das Erreichen einer Singbedingung durch Zufall für das Neuronale Netzwerk zu komplex ist und es somit keine Möglichkeit hat festzustellen wie \textit{Gut} es ist und sich anzupassen.\cite{mnih2015human}